
kysval
kysbin

make kysbin from kysval

simple kysbin
 goes from lowval to highval+eps in secval sections 
 secval= (highval-lowval)/bins
 
 highval is highest value in kysval +epsilon so that it fits
 into last section
 
underbin,lowbin,highbin,overbin

unmark  = minval     smallest value needing placed
lomark  = lowstat
himark = highstat
mxmark  = maxval+epsilon    eps more than highest val needing placed

if statsok do simple binning by
  terval = maxval-minval)/bins
  lostat=lomark = minmark + terval
  histat=himark = maxmark-terval
  
else do special binning by
  terval = maxstat-minstat)/bins
  minmark=minval (can be -Inf)
  lowmark=
  
////////////  
  
measure of range 

read ahead 1/4th to calc total binpop due

observe avg binpop of crange,  cdens
average binpop of rest         edens
and decaying binpop of pivot   pdens

increase crange while cdens is different from the due
split crange before cdens becomes similar to edens

//////////

state of project: barsort.js

it was tried: not saving quantized scores, but instead
recalculating them twice on fly, but resulted in significant
slowdown as the calculation was heavier apparently than 
the extra memory demand, even for multi megabyte lists.

however it is well not to create scores 'in place'
as score redistribution can be repeated on a saved input
to improve the fill level of buckets:



foreach bucket 
  if zero extend current-zero-span

foreach rescalebucket range{
    
  foreach rangebucket
    bucket=0

  foreach element
    if elem>rdistrange ||<rdistrange
      elem= rescale elem
      elem= quantise
      
}

///
barsort is possibly not well optimised yet
barassign has multiplexed bars in order improve the definition
of large bars.
but small bars are used to make presorted index for full sorting
most efficient may be bars with average target population of 3


#now - completing barsort fastest numeric sort on npm

  binrange stages: minimal - medium - intense
  
  scores are stat surveyed to calculate initial bin measurements
    highest val, 2nd hv, lowestval, 2nd lv, stand deviation
    
  develope first with no descending capabi

  if these are ideal do fast-binning (without clauses for side-range)
  
  if not ideal do a binning with clauses for underval and overval
  
   first binning has been done
  
  tally the bins, noting max spillages
  
  if low spillage skip to bin distribution
  
  bin tally analysis follows:
    loop through bins recording 'story'
     of low spells, high spells, peaks
     marking;
      pos low started, low continued
      pos medium implied
      pos high started, high continued
      pos high peak occured
      summary of amount of overflow involved
      
  indentify runs:
    almost empty -  
    low          - compress
    medium       - 
    high         - stretch
    spike        - 
 
  if 
     
  
  var ri=0   
  while(val>rangeAnchor[ri]) ri++
  assignto(floor((val-rangeAnchor[ri])/rangeTicks[ri])+rangeAbin[ri]   
     
     
     
  bin redrawing
   value window low to high to 
     bin range as start bin + windowpos/binwidth + special bin offset
   value window can be discrete point
     
  
  
   decides
   no refinement
   
  minimal:
   low_underrange , high_overrange , 
   
  stat survey can indicate if simplest mapper is possible
  
  
  bin count
  bin count analyse
   survey bin counts
//-------------



### Pattern Defeat

A small number of peaks of inputs values can result in the same amount of very overloaded bins, which can cause a lot of slow down. To defeat this I concieved of a fast bin rescaling proceedure --This is what has ostensibly lost me to the realms of crazy. The finished 'improvebins' function 

If a quick check of the normaly tallied bins indicates problematic
clumping, proceed with the goal of determining which input ranges to
stretch while binning and then to quickly rebin values in an 
efficient bin stretching proceedure.

Basically this is just about working, but it is not pleasing or
optimal. Extremely difficult to debug.
 

In the quest to 'perfectize' this project I first improved the auxillary insert+mergesort so it performed better with poorly presorted cases. In my 'zone of haste' those were convoluted to explicitly sort the index to the data rather than to do the indirection through the compare function which would be much more readable and possibly just as quick. The resulting insert+mergesort combo is quite fast though.


I use shortened coded variable names, to help fit proceedures onto screens.

One convention I end up using a lot is prefixing ordinal hints to indices, eg: 
```
 bbin=cbin   //before bin = current bin
 cbin=dbin   //current bin = due bin
 dbin=calc_due_bin() //new due bin

 cr_tot+=cbin  //current_range_total
 cr_len++      //current_range_length
```


The source code is currently 

//-------------------
